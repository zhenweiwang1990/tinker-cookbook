"""
Implements RL on general MDPs
"""

import asyncio
import io
import json
import logging
import os
import time
from contextlib import contextmanager
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Iterator, List, Sequence

import chz
import numpy as np
import tinker
import torch
from tinker.types import LossFnType
from tinker_cookbook import checkpoint_utils
from tinker_cookbook.completers import TinkerTokenCompleter
from tinker_cookbook.display import colorize_example
from tinker_cookbook.eval.evaluators import SamplingClientEvaluator, SamplingClientEvaluatorBuilder
from tinker_cookbook.rl.data_processing import (
    assemble_training_data,
    compute_advantages,
    remove_constant_reward_groups,
)
from tinker_cookbook.rl.metric_util import RLTestSetEvaluator, compute_trajectory_metrics
from tinker_cookbook.rl.metrics import (
    compute_kl_sample_train,
    compute_post_kl,
    compute_sampling_client_metrics,
    incorporate_kl_penalty,
)
from tinker_cookbook.rl.rollouts import do_group_rollout
from tinker_cookbook.rl.types import (
    EnvGroupBuilder,
    RLDataset,
    RLDatasetBuilder,
    TrajectoryGroup,
)
from tinker_cookbook.tokenizer_utils import Tokenizer
from tinker_cookbook.utils import logtree, ml_log
from tinker_cookbook.utils.misc_utils import safezip, split_list, timed, all_same
from tinker_cookbook.utils.trace import scope, update_scope_context, trace_init

logger = logging.getLogger(__name__)


def _get_evaluator_name(evaluator: SamplingClientEvaluator) -> str:
    return (
        evaluator.name
        if isinstance(evaluator, RLTestSetEvaluator) and evaluator.name is not None
        else ""
    )


@contextmanager
def _get_logtree_scope(
    log_path: str | None, num_groups_to_log: int, f_name: str, scope_name: str
) -> Iterator[None]:
    """
    Creates a context manager; all log inside this context will be logged under the section `scope_name`.
    It will create a file with the path of log_path/f_name.html
    If num_groups_to_log is 0, it will disable logging (but note that this function does not actually implement the logic for logging itself!)
    """
    if log_path is not None and num_groups_to_log > 0:
        logtree_path = os.path.join(log_path, f"{f_name}.html")
        with logtree.init_trace(scope_name, path=logtree_path):
            yield
    else:
        yield


@scope
def _select_representative_inds(scores: list[float], num_inds: int) -> list[int]:
    assert num_inds <= len(scores)
    sorted_inds = np.argsort(scores)
    uniform_inds = np.linspace(0, len(sorted_inds) - 1, num_inds).astype(int)
    return [int(sorted_inds[i]) for i in uniform_inds]


@scope
def print_group(traj_group: TrajectoryGroup, tokenizer: Tokenizer):
    """
    Print a subset of the trajectory group to the console.
    """
    # Cut down the number of trajectories to print
    max_trajs_to_print = 4
    if len(traj_group.trajectories_G) > max_trajs_to_print:
        inds = _select_representative_inds(traj_group.get_total_rewards(), max_trajs_to_print)
        traj_group = TrajectoryGroup(
            trajectories_G=[traj_group.trajectories_G[i] for i in inds],
            final_rewards_G=[traj_group.final_rewards_G[i] for i in inds],
            metrics_G=[traj_group.metrics_G[i] for i in inds],
        )

    rewards = traj_group.get_total_rewards()
    advantages_G = compute_advantages([traj_group])
    data_D, metadata_D = assemble_training_data([traj_group], advantages_G)

    buf = io.StringIO()

    @scope
    def bprint(s: str):
        print(s, file=buf)

    bprint("\n====== Trajectory Group ======")
    last_metadata = None
    for datum, metadata in safezip(data_D, metadata_D):
        idx = metadata["traj_idx"]
        if metadata != last_metadata:
            bprint(f"****** trajectory idx={idx}, reward={rewards[idx]:.3g} ******")
            # Print trajectory-level metrics
            if traj_group.metrics_G[idx]:
                bprint("Trajectory metrics:")
                for key, value in traj_group.metrics_G[idx].items():
                    bprint(f"  {key}: {value}")
            # Print per-transition metrics
            transition_metrics = [
                transition.metrics
                for transition in traj_group.trajectories_G[idx].transitions
                if transition.metrics
            ]
            if transition_metrics:
                bprint("Per-step metrics:")
                for i, metrics in enumerate(transition_metrics):
                    bprint(f"  Step {i}:")
                    for key, value in metrics.items():
                        bprint(f"    {key}: {value}")
        bprint("---- datum ----")
        bprint(colorize_example(datum, tokenizer, key="advantages"))
        last_metadata = metadata
    bprint("====== End Trajectory Group ======")
    logger.info(buf.getvalue().rstrip())


def _remove_mask(datum: tinker.Datum) -> tinker.Datum:
    return tinker.Datum(
        model_input=datum.model_input,
        loss_fn_inputs={k: v for k, v in datum.loss_fn_inputs.items() if k != "mask"},
    )


def _training_logprobs_from_fwd_bwd(
    fwd_bwd_result: tinker.ForwardBackwardOutput,
) -> list[torch.Tensor]:
    return [output["logprobs"].to_torch() for output in fwd_bwd_result.loss_fn_outputs]


@scope
async def train_step(
    data_D: List[tinker.Datum],
    training_client: tinker.TrainingClient,
    learning_rate: float,
    num_substeps: int,
    loss_fn: LossFnType,
) -> List[torch.Tensor]:
    """Train the model on collected trajectories.

    Pipelines forward_backward and optim_step so they land on the same clock cycle.
    """
    batches = split_list(data_D, min(num_substeps, len(data_D)))
    if not batches:
        return []

    adam_params = tinker.AdamParams(learning_rate=learning_rate, beta1=0.9, beta2=0.95, eps=1e-8)
    training_logprobs_D: list[torch.Tensor] = []

    # Enqueue first batch
    fwd_bwd_future = await training_client.forward_backward_async(
        [_remove_mask(d) for d in batches[0]], loss_fn=loss_fn
    )
    optim_future = await training_client.optim_step_async(adam_params)

    for i in range(len(batches)):
        # Enqueue next batch before consuming current results (to stay on same clock cycle)
        if i + 1 < len(batches):
            next_fwd_bwd_future = await training_client.forward_backward_async(
                [_remove_mask(d) for d in batches[i + 1]], loss_fn=loss_fn
            )
            next_optim_future = await training_client.optim_step_async(adam_params)
        else:
            next_fwd_bwd_future = None
            next_optim_future = None
        # Consume current results
        fwd_bwd_result = await fwd_bwd_future.result_async()
        training_logprobs_D.extend(_training_logprobs_from_fwd_bwd(fwd_bwd_result))
        await optim_future.result_async()
        # Move to next iteration
        if next_fwd_bwd_future is not None and next_optim_future is not None:
            fwd_bwd_future = next_fwd_bwd_future
            optim_future = next_optim_future

    return training_logprobs_D


@chz.chz
class StreamMinibatchConfig:
    """
    Configuration for training with minibatch streaming.
    Once we have accumulated enough trajectories for a minibatch, we will
    immediately train on them, instead of waiting for the full batch of
    trajectories to be ready.
    """

    # Total number of trajectory groups across all minibatches and substeps
    groups_per_batch: int
    # For each substep, we will divide up the number of trajectory groups
    # into this many minibatches.
    # We will do num_minibatches forward_backward() passes and one optim_step()
    # per substep.
    num_minibatches: int


@chz.chz
class AsyncConfig:
    """Configuration for async RL training"""

    # If samples are generated from a sample more than this many steps ago,
    # we will skip training on them.
    max_steps_off_policy: int
    # We will ensure all batches have at least this many groups, even
    # as we discard stale samples
    groups_per_batch: int


@chz.chz
class Config:
    learning_rate: float
    dataset_builder: RLDatasetBuilder  # also determines batch size
    model_name: str
    max_tokens: int
    temperature: float = 1.0  # Changing sampling temperature is not generally recommended; does not currently play well with KL penalty
    compute_post_kl: bool = False
    evaluator_builders: list[SamplingClientEvaluatorBuilder] = chz.field(default_factory=list)
    lora_rank: int = 32

    kl_penalty_coef: float = 0.0
    kl_discount_factor: float = 0.0

    # Loss function to use for training: "importance_sampling" or "ppo"
    loss_fn: LossFnType = "importance_sampling"

    # Number of optimizer steps per training iteration.
    # Useful for very large batch sizes.
    num_substeps: int = 1

    wandb_project: str | None = None
    wandb_name: str | None = None

    log_path: str = chz.field(munger=lambda _, s: os.path.expanduser(s))
    base_url: str | None = None
    enable_trace: bool = False

    remove_constant_reward_groups: bool = False
    eval_every: int = 20  # 0 = disabled
    save_every: int = 20  # 0 = disabled
    load_checkpoint_path: str | None = None
    skip_baseline: bool = False  # If True, skip baseline evaluation and start training directly
    baseline_only: bool = False  # If True, only run baseline evaluation and exit (no training)

    async_config: AsyncConfig | None = None
    stream_minibatch_config: StreamMinibatchConfig | None = None

    # Logtree configuration
    num_groups_to_log: int = 4  # Number of groups to log per iteration (0 = disable logging)


@scope
async def run_single_evaluation(evaluator, cfg, i_batch, sampling_client):
    ev_name = _get_evaluator_name(evaluator)
    with _get_logtree_scope(
        log_path=cfg.log_path,
        num_groups_to_log=cfg.num_groups_to_log,
        f_name=f"eval_{ev_name}_iteration_{i_batch:06d}",
        scope_name=f"Running evaluation {ev_name} {i_batch}",
    ):
        eval_metrics = await evaluator(sampling_client)
        return eval_metrics


@scope
async def run_single_baseline_evaluation(evaluator, cfg, sampling_client):
    """Run a single baseline evaluation (before training starts)."""
    ev_name = _get_evaluator_name(evaluator)
    with _get_logtree_scope(
        log_path=cfg.log_path,
        num_groups_to_log=cfg.num_groups_to_log,
        f_name=f"eval_{ev_name}_baseline",
        scope_name=f"Running baseline evaluation {ev_name}",
    ):
        eval_metrics = await evaluator(sampling_client)
        return eval_metrics


@scope
async def run_evaluations_parallel(
    evaluators: list[SamplingClientEvaluator],
    sampling_client: tinker.SamplingClient,
    cfg: Config,
    i_batch: int,
    model_path: str | None = None,
) -> dict[str, Any]:
    """Run all evaluators in parallel and return aggregated metrics."""

    # Early return if no evaluators
    if len(evaluators) == 0:
        return {}
    
    # Set model_path for evaluation rollouts (for CUA custom rollout function)
    if model_path is not None:
        try:
            from tinker_cookbook.recipes.cua_rl.train import set_eval_model_path
            set_eval_model_path(model_path)
        except ImportError:
            # Not using CUA RL, skip
            pass

    # Create tasks for all evaluators with names for better traceability
    tasks = []
    for i, evaluator in enumerate(evaluators):
        ev_name = _get_evaluator_name(evaluator)
        task = asyncio.create_task(
            run_single_evaluation(evaluator, cfg, i_batch, sampling_client),
            name=f"eval_{ev_name or i}_iteration_{i_batch:06d}",
        )
        tasks.append(task)

    # Wait for all to complete
    results = await asyncio.gather(*tasks)

    # Merge all metrics
    metrics = {}
    for result in results:
        metrics.update(result)
    
    # Clear model_path after evaluation
    if model_path is not None:
        try:
            from tinker_cookbook.recipes.cua_rl.train import set_eval_model_path
            set_eval_model_path(None)
        except ImportError:
            pass

    return metrics


@scope
async def run_baseline_evaluations_parallel(
    evaluators: list[SamplingClientEvaluator],
    sampling_client: tinker.SamplingClient,
    cfg: Config,
    model_path: str | None = None,
) -> dict[str, Any]:
    """Run all evaluators in parallel for baseline evaluation (before training starts)."""

    # Early return if no evaluators
    if len(evaluators) == 0:
        return {}
    
    # Set model_path for evaluation rollouts (for CUA custom rollout function)
    if model_path is not None:
        try:
            from tinker_cookbook.recipes.cua_rl.train import set_eval_model_path
            set_eval_model_path(model_path)
        except ImportError:
            # Not using CUA RL, skip
            pass

    # Create tasks for all evaluators with names for better traceability
    tasks = []
    for i, evaluator in enumerate(evaluators):
        ev_name = _get_evaluator_name(evaluator)
        task = asyncio.create_task(
            run_single_baseline_evaluation(evaluator, cfg, sampling_client),
            name=f"eval_{ev_name or i}_baseline",
        )
        tasks.append(task)

    # Wait for all to complete
    results = await asyncio.gather(*tasks)

    # Merge all metrics
    metrics = {}
    for result in results:
        metrics.update(result)
    
    # Clear model_path after evaluation
    if model_path is not None:
        try:
            from tinker_cookbook.recipes.cua_rl.train import set_eval_model_path
            set_eval_model_path(None)
        except ImportError:
            pass

    return metrics


@scope
async def do_sync_training_with_stream_minibatch(
    start_batch: int,
    end_batch: int,
    num_batches: int,
    cfg: Config,
    training_client: tinker.TrainingClient,
    service_client: tinker.ServiceClient,
    evaluators: list[SamplingClientEvaluator],
    dataset: RLDataset,
    ml_logger: ml_log.Logger,
    tokenizer: Tokenizer,
):
    """
    Implements fully synchronous on-policy training with minibatch streaming.
    Once we have accumulated enough trajectories for a minibatch, we will
    immediately train on them, instead of waiting for the full batch of
    trajectories to be ready. This allows us to overlap sampling and training.
    """
    # Initial sampling client
    sampling_client, model_path, _ = await save_checkpoint_and_get_sampling_client(
        training_client, start_batch, cfg.log_path, cfg.save_every, start_batch
    )

    for i_batch in range(start_batch, end_batch):
        metrics = {
            "progress/batch": i_batch,
            "optim/lr": cfg.learning_rate,
            "progress/done_frac": (i_batch + 1) / num_batches,
        }
        t_start = time.time()

        # Run evaluations
        # Skip step 0 evaluation if baseline was run (start_batch == 0 means baseline ran)
        if ((cfg.eval_every > 0 and i_batch % cfg.eval_every == 0) or i_batch == end_batch - 1) and not (i_batch == 0 and start_batch == 0):
            eval_start = time.time()
            with timed("run_evals", metrics):
                eval_metrics = await run_evaluations_parallel(
                    evaluators, sampling_client, cfg, i_batch, model_path=model_path
                )
                metrics.update(eval_metrics)
            eval_time = time.time() - eval_start
            
            # Save evaluation results to a separate JSON file for easy reference
            eval_results_file = Path(cfg.log_path) / f"eval_results_batch_{i_batch:06d}.json"
            eval_results = {
                "batch": i_batch,
                "evaluation_time_seconds": eval_time,
                "timestamp": datetime.now().isoformat(),
                "metrics": eval_metrics,
            }
            with open(eval_results_file, "w") as f:
                json.dump(eval_results, f, indent=2)
            logger.info(f"Evaluation results saved to {eval_results_file.name}")

        with _get_logtree_scope(
            cfg.log_path,
            cfg.num_groups_to_log,
            f"train_iteration_{i_batch:06d}",
            f"RL Iteration {i_batch}",
        ):
            # Samplers will produce trajectory groups asynchronously,
            # and the trainer will consume them as soon as they are ready
            trajectory_groups_queue = asyncio.Queue[WrappedTrajectoryGroup | None]()
            env_group_builders_P = dataset.get_batch(i_batch)

            @scope
            async def trajectory_group_worker_task(
                builder: EnvGroupBuilder, enable_logging: bool
            ) -> None:
                metrics = {}
                t_start = time.time()
                trajectory_group = await do_group_rollout_and_filter_constant_reward(
                    sampling_client,
                    builder,
                    max_tokens=cfg.max_tokens,
                    temperature=cfg.temperature,
                    do_remove_constant_reward_groups=cfg.remove_constant_reward_groups,
                    enable_logging=enable_logging,
                    model_path=model_path,
                )
                metrics["time/trajectory_group_worker_loop/total"] = time.time() - t_start
                if trajectory_group is not None:
                    trajectory_groups_queue.put_nowait(
                        WrappedTrajectoryGroup(
                            trajectory_group=trajectory_group,
                            env_group_builder=builder,
                            sampling_client_step=i_batch,
                            metrics=metrics,
                        )
                    )
                else:
                    trajectory_groups_queue.put_nowait(None)

            # Sample all trajectories asynchronously. If we have multiple minibatches,
            # then sampling can overlap with training.
            for i, builder in enumerate(env_group_builders_P):
                asyncio.create_task(
                    trajectory_group_worker_task(builder, enable_logging=i < cfg.num_groups_to_log),
                    name=f"trajectory_group_worker_task_{i}",
                )

            # Run multiple optimizer substeps per training iteration
            (
                sampling_client,
                model_path,
                full_batch_metrics,
            ) = await do_train_step_streaming_and_get_sampling_client(
                cfg,
                i_batch,
                trajectory_groups_queue,
                training_client,
                service_client,
                tokenizer,
            )

        # Log metrics
        metrics.update(full_batch_metrics)
        metrics["time/total"] = time.time() - t_start
        ml_logger.log_metrics(metrics, step=i_batch)


@chz.chz
class WrappedTrajectoryGroup:
    """
    A wrapper around a trajectory group that includes metadata about how it was generated.
    Used when we need to overlap sampling and training.
    """

    trajectory_group: TrajectoryGroup
    # The env group builder that produced the trajectory group.
    # Pass this along in case the sampler is too stale, and we need to
    # requeue this group.
    env_group_builder: EnvGroupBuilder
    # The step that produced this trajectory group.
    sampling_client_step: int
    metrics: dict[str, Any] = chz.field(default_factory=dict)


@scope
async def do_async_training(
    start_batch: int,
    end_batch: int,
    num_batches: int,
    cfg: Config,
    training_client: tinker.TrainingClient,
    service_client: tinker.ServiceClient,
    evaluators: list[SamplingClientEvaluator],
    dataset: RLDataset,
    ml_logger: ml_log.Logger,
    tokenizer: Tokenizer,
):
    """Implements async off-policy training, capped at K steps off policy."""
    assert cfg.async_config is not None

    shutdown_event = asyncio.Event()
    # We will have groups_per_batch worker generating rollouts, so cap the
    # queue size to be groups_per_batch.
    env_group_builders_queue = asyncio.Queue[EnvGroupBuilder | None](
        maxsize=cfg.async_config.groups_per_batch
    )
    trajectory_groups_queue = asyncio.Queue[WrappedTrajectoryGroup | None]()

    # Initial sampling client to use
    path_dict = await checkpoint_utils.save_checkpoint_async(
        training_client=training_client,
        name=f"{start_batch:06d}",
        log_path=cfg.log_path,
        loop_state={"batch": start_batch},
        kind="both",
    )

    # This will be updated by the training loop
    sampling_client = training_client.create_sampling_client(path_dict["sampler_path"])
    sampling_client_step = start_batch
    sampling_client_updated_event = asyncio.Event()
    sampling_client_updated_event.set()

    @scope
    def shutdown_loops():
        """Trigger all loops to shutdown"""
        shutdown_event.set()
        assert cfg.async_config is not None
        for _ in range(cfg.async_config.groups_per_batch):
            env_group_builders_queue.put_nowait(None)
        sampling_client_updated_event.set()

    @scope
    async def dataloader_loop():
        """Gets the next set of env builders to run"""
        i_batch = start_batch
        while not shutdown_event.is_set() and i_batch < end_batch:
            env_group_builders_P = dataset.get_batch(i_batch)
            for env_group_builder in env_group_builders_P:
                await env_group_builders_queue.put(env_group_builder)
            i_batch += 1

    @scope
    async def trajectory_group_worker_loop():
        """Generates trajectories for a single env builder"""
        while not shutdown_event.is_set():
            env_group_builder = await env_group_builders_queue.get()
            if env_group_builder is None:
                break

            metrics = {}
            t_start = time.time()
            # Save a reference to the sampling client step in case it changes
            # while we're running the rollout
            sampling_client_step_copy = sampling_client_step
            trajectory_group = await do_group_rollout_and_filter_constant_reward(
                sampling_client,
                env_group_builder,
                max_tokens=cfg.max_tokens,
                temperature=cfg.temperature,
                do_remove_constant_reward_groups=cfg.remove_constant_reward_groups,
            )
            if trajectory_group is None:
                trajectory_groups_queue.put_nowait(None)
            else:
                metrics["time/trajectory_group_worker_loop/total"] = time.time() - t_start
                trajectory_groups_queue.put_nowait(
                    WrappedTrajectoryGroup(
                        trajectory_group=trajectory_group,
                        env_group_builder=env_group_builder,
                        sampling_client_step=sampling_client_step_copy,
                        metrics=metrics,
                    )
                )

    @scope
    async def training_loop():
        """
        Waits for a sufficient number of valid trajectories to be accumulated and trains on them.
        Will discard trajectories that are too stale.
        """
        assert cfg.async_config is not None

        i_batch = start_batch
        wrapped_trajectory_groups = []
        while i_batch < end_batch:
            wrapped_trajectory_group = await trajectory_groups_queue.get()
            if wrapped_trajectory_group is None:
                continue

            @scope
            def filter_stale_trajectory_group(
                wrapped_trajectory_group: WrappedTrajectoryGroup | None,
            ) -> bool:
                """Returns False if the trajectory group is too stale or not valid"""
                if wrapped_trajectory_group is None:
                    return False

                # If the samples are too stale, requeue the data so that it will be used eventually.
                # Requeue on a separate coroutine to avoid blocking the training loop
                assert cfg.async_config is not None
                if (
                    i_batch - wrapped_trajectory_group.sampling_client_step
                    > cfg.async_config.max_steps_off_policy
                ):
                    logger.info(f"[training_loop] Step {i_batch}: Samples are too stale, skipping")
                    asyncio.create_task(
                        env_group_builders_queue.put(wrapped_trajectory_group.env_group_builder),
                        name="requeue_stale_sample_task",
                    )
                    return False
                return True

            metrics = {
                "training_client/step": i_batch,
                "optim/lr": cfg.learning_rate,
                "progress/done_frac": (i_batch + 1) / num_batches,
            }
            t_start = time.time()

            nonlocal sampling_client
            nonlocal sampling_client_step
            if cfg.stream_minibatch_config is not None:
                await trajectory_groups_queue.put(wrapped_trajectory_group)
                (
                    sampling_client,
                    model_path,
                    train_step_metrics,
                ) = await do_train_step_streaming_and_get_sampling_client(
                    cfg,
                    i_batch,
                    trajectory_groups_queue,
                    training_client,
                    service_client,
                    tokenizer,
                    filter_stale_trajectory_group,
                )
            else:
                if not filter_stale_trajectory_group(wrapped_trajectory_group):
                    continue

                # Dynamic sampling: Wait for enough trajectories to accumulate to
                # ensure all batch sizes are the same size. This avoids needing to adjust
                # the learning rate for different batch sizes.
                wrapped_trajectory_groups.append(wrapped_trajectory_group)
                if len(wrapped_trajectory_groups) < cfg.async_config.groups_per_batch:
                    continue
                logger.info(
                    f"[training_loop] Step {i_batch}: Will train on batch, num groups: {len(wrapped_trajectory_groups)}"
                )

                # Compute sampling client metrics, as samples may have been generated with
                # different sampler versions
                metrics.update(compute_sampling_client_metrics(wrapped_trajectory_groups))

                # TODO: For proper checkpointing, we also need to save dataloader state and
                # all queued trajectory groups that haven't been trained on yet
                sampling_client, train_step_metrics = await do_train_step_and_get_sampling_client(
                    cfg,
                    i_batch,
                    training_client,
                    service_client,
                    tokenizer,
                    [g.env_group_builder for g in wrapped_trajectory_groups],
                    [g.trajectory_group for g in wrapped_trajectory_groups],
                )
            sampling_client_step = i_batch + 1
            sampling_client_updated_event.set()

            # Log metrics
            metrics.update(train_step_metrics)
            metrics["time/training_loop/total"] = time.time() - t_start
            ml_logger.log_metrics(metrics, step=i_batch)
            i_batch += 1
            wrapped_trajectory_groups = []

        shutdown_loops()

    @scope
    async def evaluation_loop():
        """Runs evals periodically"""
        if len(evaluators) == 0 or cfg.eval_every == 0:
            return

        while not shutdown_event.is_set():
            await sampling_client_updated_event.wait()
            sampling_client_updated_event.clear()

            metrics = {}
            t_start = time.time()
            # Save a reference to the original values in case it changes
            # while we're running the evals
            sampling_client_eval_step = sampling_client_step
            sampling_client_eval = sampling_client
            # Skip step 0 evaluation if baseline was run (start_batch == 0 means baseline ran)
            if cfg.eval_every > 0 and sampling_client_eval_step % cfg.eval_every == 0 and not (sampling_client_eval_step == 0 and start_batch == 0):
                logger.info("")
                logger.info("╔" + "=" * 78 + "╗")
                logger.info(f"║ EVALUATION - Step {sampling_client_eval_step}" + " " * (78 - len(f"EVALUATION - Step {sampling_client_eval_step}")) + "║")
                logger.info("╠" + "=" * 78 + "╣")
                logger.info(f"║ Running {len(evaluators)} evaluator(s)..." + " " * (78 - len(f"Running {len(evaluators)} evaluator(s)...")) + "║")
                with timed("run_evals", metrics):
                    for eval_idx, evaluator in enumerate(evaluators):
                        eval_name = _get_evaluator_name(evaluator) or f"Evaluator {eval_idx + 1}"
                        logger.info(f"║   Running {eval_name}..." + " " * (78 - len(f"  Running {eval_name}...")) + "║")
                        eval_start = time.time()
                        eval_metrics = await evaluator(sampling_client_eval)
                        eval_time = time.time() - eval_start
                        logger.info(f"║   ✓ {eval_name} completed in {eval_time:.2f}s" + " " * (78 - len(f"  ✓ {eval_name} completed in {eval_time:.2f}s")) + "║")
                        for key, value in eval_metrics.items():
                            logger.info(f"║     {key}: {value}" + " " * (78 - len(f"    {key}: {value}")) + "║")
                        metrics.update({f"test/{k}": v for k, v in eval_metrics.items()})
                metrics["time/evaluation_loop/total"] = time.time() - t_start
                logger.info(f"║ Total evaluation time: {time.time() - t_start:.2f}s" + " " * (78 - len(f"Total evaluation time: {time.time() - t_start:.2f}s")) + "║")
                logger.info("╚" + "=" * 78 + "╝")
                ml_logger.log_metrics(metrics, step=sampling_client_eval_step)

    await asyncio.gather(
        asyncio.create_task(dataloader_loop(), name="dataloader_loop"),
        *[
            asyncio.create_task(
                trajectory_group_worker_loop(), name=f"trajectory_group_worker_loop_{i}"
            )
            for i in range(cfg.async_config.groups_per_batch)
        ],
        asyncio.create_task(training_loop(), name="training_loop"),
        asyncio.create_task(evaluation_loop(), name="evaluation_loop"),
    )


@scope
async def do_group_rollout_and_filter_constant_reward(
    sampling_client: tinker.SamplingClient,
    env_group_builder: EnvGroupBuilder,
    max_tokens: int,
    temperature: float,
    do_remove_constant_reward_groups: bool,
    enable_logging: bool = True,
    model_path: str | None = None,
    group: int | None = None,
) -> TrajectoryGroup | None:
    policy = TinkerTokenCompleter(sampling_client, max_tokens=max_tokens, temperature=temperature)

    with logtree.optional_enable_logging(enable_logging):
        # Pass model_path and group to do_group_rollout if it accepts the parameters
        # Check if the function signature includes these parameters
        import inspect
        sig = inspect.signature(do_group_rollout)
        kwargs = {}
        if 'model_path' in sig.parameters:
            kwargs['model_path'] = model_path
        if 'group' in sig.parameters:
            kwargs['group'] = group
        trajectory_group = await do_group_rollout(env_group_builder, policy, **kwargs)

    # Remove if all trajectories have the same reward
    if do_remove_constant_reward_groups and all_same(trajectory_group.get_total_rewards()):
        return None
    else:
        return trajectory_group


@scope
async def save_checkpoint_and_get_sampling_client(
    training_client: tinker.TrainingClient,
    i_batch: int,
    log_path: str,
    save_every: int,
    start_batch: int = 0,
) -> tuple[tinker.SamplingClient, str, dict[str, Any]]:
    metrics = {}
    with timed("save_checkpoint", metrics):
        if save_every > 0 and i_batch > start_batch and i_batch % save_every == 0:
            path_dict = await checkpoint_utils.save_checkpoint_async(
                training_client=training_client,
                name=f"{i_batch:06d}",
                log_path=log_path,
                loop_state={"batch": i_batch},
                kind="both",
            )
            sampler_path = path_dict["sampler_path"]
            return training_client.create_sampling_client(sampler_path), sampler_path, metrics
        else:
            # For non-checkpoint batches, still save with explicit path for on-policy rollout
            path_dict = await checkpoint_utils.save_checkpoint_async(
                training_client=training_client,
                name=f"{i_batch:06d}_tmp",
                log_path=log_path,
                loop_state={},
                kind="sampler",
            )
            sampler_path = path_dict["sampler_path"]
            return training_client.create_sampling_client(sampler_path), sampler_path, metrics


@scope
async def prepare_minibatch(
    env_group_builders_P: Sequence[EnvGroupBuilder],
    trajectory_groups_P: list[TrajectoryGroup],
    tokenizer: Tokenizer,
    service_client: tinker.ServiceClient,
    model_name: str,
    kl_penalty_coef: float,
    kl_discount_factor: float,
) -> tuple[list[tinker.Datum], dict[str, Any]]:
    """Converts the trajectories into a minibatch, and provides metrics about the minibatch"""

    # Compute trajectory metrics
    metrics = {}
    taglist_P = [env_group_builder.logging_tags() for env_group_builder in env_group_builders_P]
    metrics.update(compute_trajectory_metrics(trajectory_groups_P, taglist_P))

    # Print up to two trajectory groups
    for traj_group in trajectory_groups_P[:2]:
        print_group(traj_group, tokenizer)

    # Assemble training data
    with timed("assemble_training_data", metrics):
        advantages_P = compute_advantages(trajectory_groups_P)
        data_D, _metadata_D = assemble_training_data(trajectory_groups_P, advantages_P)

    # Incorporate KL penalty if configured
    if kl_penalty_coef > 0:
        with timed("kl_vs_base", metrics):
            kl_penalty_metrics = await incorporate_kl_penalty(
                data_D,
                service_client.create_sampling_client(base_model=model_name),
                # ^^^ TODO: replace with the model we load, if relevant
                kl_penalty_coef,
                kl_discount_factor,
            )
        metrics.update(kl_penalty_metrics)

    return data_D, metrics


@scope
async def compute_full_batch_metrics_and_get_sampling_client(
    training_client: tinker.TrainingClient,
    i_batch: int,
    data_D: list[tinker.Datum],
    training_logprobs_D: list[torch.Tensor],
    log_path: str,
    save_every: int,
    do_compute_post_kl: bool,
) -> tuple[tinker.SamplingClient, str, dict[str, Any]]:
    """
    At the end of the iteration, this will compute metrics for the full batch
    and return the latest sampling client with its model_path.

    The reason we return a sampling client is that if do_compute_post_kl is True,
    we need to create a sampling client from the post-update policy.
    """
    metrics = {}

    # Compute KL metrics
    with timed("compute_kl_sample_train", metrics):
        kl_sample_train_metrics = compute_kl_sample_train(data_D, training_logprobs_D)
        metrics.update(kl_sample_train_metrics)

    # Get a sampling client using the new weights
    sampling_client, model_path, checkpoint_metrics = await save_checkpoint_and_get_sampling_client(
        training_client, i_batch, log_path, save_every
    )
    metrics.update(checkpoint_metrics)

    # Compute post-KL metrics if configured
    if do_compute_post_kl:
        with timed("compute_post_kl", metrics):
            post_kl_metrics = await compute_post_kl(data_D, sampling_client)
            metrics.update(post_kl_metrics)

    return sampling_client, model_path, metrics


@scope
async def do_train_step_streaming_and_get_sampling_client(
    cfg: Config,
    i_batch: int,
    trajectory_groups_queue: asyncio.Queue[WrappedTrajectoryGroup | None],
    training_client: tinker.TrainingClient,
    service_client: tinker.ServiceClient,
    tokenizer: Tokenizer,
    trajectory_group_filter: Callable[[WrappedTrajectoryGroup | None], bool] = lambda _: True,
) -> tuple[tinker.SamplingClient, str, dict[str, Any]]:
    """
    As soon as we have enough trajectories for a minibatch, we will train on them.
    This allows us to overlap sampling and training.
    """
    assert cfg.stream_minibatch_config is not None
    assert cfg.stream_minibatch_config.groups_per_batch % cfg.num_substeps == 0, (
        f"{cfg.stream_minibatch_config.groups_per_batch=} must be divisible by {cfg.num_substeps=}"
    )
    # Number of groups across all minibatches in each optimizer substep
    groups_per_substep = cfg.stream_minibatch_config.groups_per_batch // cfg.num_substeps
    assert groups_per_substep % cfg.stream_minibatch_config.num_minibatches == 0, (
        f"{groups_per_substep} must be divisible by {cfg.stream_minibatch_config.num_minibatches=}"
    )
    # Number of groups per minibatch in each optimizer substep
    groups_per_minibatch = groups_per_substep // cfg.stream_minibatch_config.num_minibatches

    update_scope_context({"step": i_batch})

    metrics = {}

    # Run multiple optimizer substeps per training iteration
    all_data_D = []
    all_training_logprobs_D = []
    all_wrapped_trajectory_groups = []
    for i_substep in range(cfg.num_substeps):
        # Run multiple minibatches per substep
        # Once we have enough trajectories for a minibatch, train on them
        wrapped_trajectory_groups = []
        forward_backward_futures: list[tinker.APIFuture[tinker.ForwardBackwardOutput]] = []
        i_minibatch = 0
        while i_minibatch < cfg.stream_minibatch_config.num_minibatches:
            wrapped_trajectory_group = await trajectory_groups_queue.get()
            if not trajectory_group_filter(wrapped_trajectory_group):
                continue
            wrapped_trajectory_groups.append(wrapped_trajectory_group)

            if len(wrapped_trajectory_groups) < groups_per_minibatch:
                continue
            logger.info(
                f"[stream_minibatch] Step {i_batch}, Substep {i_substep}/{cfg.num_substeps}, Minibatch {i_minibatch}/{cfg.stream_minibatch_config.num_minibatches}: Will train on minibatch, num groups: {len(wrapped_trajectory_groups)}"
            )

            # Note: we may have removed trajectory groups that have the same reward.
            # To have the same results as the sync implementation, we will
            # remove these and train on a smaller batch.
            wrapped_trajectory_groups = [g for g in wrapped_trajectory_groups if g is not None]
            if len(wrapped_trajectory_groups) == 0:
                i_minibatch += 1
                continue

            data_D, prepare_minibatch_metrics = await prepare_minibatch(
                [g.env_group_builder for g in wrapped_trajectory_groups],
                [g.trajectory_group for g in wrapped_trajectory_groups],
                tokenizer,
                service_client,
                model_name=cfg.model_name,
                kl_penalty_coef=cfg.kl_penalty_coef,
                kl_discount_factor=cfg.kl_discount_factor,
            )
            metrics.update(prepare_minibatch_metrics)

            # Enqueue forward-backward (we'll await results after all minibatches are enqueued)
            with timed(f"train/fwd_bwd_substep_{i_substep}_mb_{i_minibatch}_enqueue", metrics):
                forward_backward_futures.append(
                    await training_client.forward_backward_async(
                        [_remove_mask(d) for d in data_D], loss_fn=cfg.loss_fn
                    )
                )
            all_data_D.extend(data_D)
            all_wrapped_trajectory_groups.extend(wrapped_trajectory_groups)
            i_minibatch += 1
            wrapped_trajectory_groups = []

        # Enqueue optim_step before awaiting results (so they land on same clock cycle)
        adam_params = tinker.AdamParams(
            learning_rate=cfg.learning_rate, beta1=0.9, beta2=0.95, eps=1e-8
        )
        with timed(f"train/optim_substep_{i_substep}_enqueue", metrics):
            optim_future = await training_client.optim_step_async(adam_params)

        # Now consume all forward-backward results
        for i_mb, fwd_bwd_future in enumerate(forward_backward_futures):
            with timed(f"train/fwd_bwd_substep_{i_substep}_mb_{i_mb}_consume", metrics):
                fwd_bwd_result = await fwd_bwd_future.result_async()
                all_training_logprobs_D.extend(_training_logprobs_from_fwd_bwd(fwd_bwd_result))

        with timed(f"train/optim_substep_{i_substep}_consume", metrics):
            await optim_future.result_async()

    # Aggregate metrics across the entire batch
    metrics.update(compute_sampling_client_metrics(all_wrapped_trajectory_groups))
    metrics.update(
        compute_trajectory_metrics(
            [g.trajectory_group for g in all_wrapped_trajectory_groups],
            [g.env_group_builder.logging_tags() for g in all_wrapped_trajectory_groups],
        )
    )
    (
        sampling_client,
        model_path,
        full_batch_metrics,
    ) = await compute_full_batch_metrics_and_get_sampling_client(
        training_client,
        # NOTE: saving the checkpoint as the i + 1 step
        i_batch + 1,
        all_data_D,
        all_training_logprobs_D,
        cfg.log_path,
        cfg.save_every,
        cfg.compute_post_kl,
    )
    metrics.update(full_batch_metrics)
    return sampling_client, model_path, metrics


@scope
async def do_train_step_and_get_sampling_client(
    cfg: Config,
    i_batch: int,
    training_client: tinker.TrainingClient,
    service_client: tinker.ServiceClient,
    tokenizer: Tokenizer,
    env_group_builders_P: Sequence[EnvGroupBuilder],
    trajectory_groups_P: list[TrajectoryGroup],
) -> tuple[tinker.SamplingClient, str, dict[str, Any]]:
    update_scope_context({"step": i_batch})

    metrics = {}
    data_D, prepare_minibatch_metrics = await prepare_minibatch(
        env_group_builders_P,
        trajectory_groups_P,
        tokenizer,
        service_client,
        model_name=cfg.model_name,
        kl_penalty_coef=cfg.kl_penalty_coef,
        kl_discount_factor=cfg.kl_discount_factor,
    )
    metrics.update(prepare_minibatch_metrics)

    with timed("train", metrics):
        training_logprobs_D = await train_step(
            data_D,
            training_client,
            cfg.learning_rate,
            cfg.num_substeps,
            cfg.loss_fn,
        )

    sampling_client, model_path, full_batch_metrics = await compute_full_batch_metrics_and_get_sampling_client(
        training_client,
        # NOTE: saving the checkpoint as the i + 1 step
        i_batch + 1,
        data_D,
        training_logprobs_D,
        cfg.log_path,
        cfg.save_every,
        cfg.compute_post_kl,
    )
    metrics.update(full_batch_metrics)

    return sampling_client, model_path, metrics


@scope
async def do_sync_training(
    start_batch: int,
    end_batch: int,
    num_batches: int,
    cfg: Config,
    training_client: tinker.TrainingClient,
    service_client: tinker.ServiceClient,
    evaluators: list[SamplingClientEvaluator],
    dataset: RLDataset,
    ml_logger: ml_log.Logger,
    tokenizer: Tokenizer,
):
    """Implements fully synchronous on-policy training"""
    logger.info("")
    logger.info("=" * 80)
    logger.info("TRAINING START")
    logger.info("=" * 80)
    logger.info(f"Start batch: {start_batch}")
    logger.info(f"End batch: {end_batch}")
    logger.info(f"Total batches: {num_batches}")
    logger.info(f"Learning rate: {cfg.learning_rate}")
    logger.info(f"Eval every: {cfg.eval_every}")
    logger.info(f"Save every: {cfg.save_every}")
    logger.info("=" * 80)
    
    # Initial sampling client
    logger.info(f"[Training Setup] Creating initial sampling client...")
    init_start = time.time()
    sampling_client, model_path, _ = await save_checkpoint_and_get_sampling_client(
        training_client, start_batch, cfg.log_path, cfg.save_every, start_batch
    )
    init_time = time.time() - init_start
    logger.info(f"[Training Setup] ✓ Initial sampling client created in {init_time:.3f}s")
    logger.info(f"[Training Setup] Initial model path: {model_path}")

    # Store previous evaluation metrics for comparison
    previous_eval_metrics: dict[str, Any] = {}
    
    for i_batch in range(start_batch, end_batch):
        batch_start_time = time.time()
        logger.info("")
        logger.info("╔" + "=" * 78 + "╗")
        logger.info(f"║ Training Step {i_batch}/{num_batches - 1} ({(i_batch + 1) / num_batches * 100:.1f}% complete)" + " " * (78 - len(f"Training Step {i_batch}/{num_batches - 1} ({(i_batch + 1) / num_batches * 100:.1f}% complete)")) + "║")
        logger.info("╠" + "=" * 78 + "╣")
        
        metrics = {
            "progress/batch": i_batch,
            "optim/lr": cfg.learning_rate,
            "progress/done_frac": (i_batch + 1) / num_batches,
        }
        t_start = time.time()

        # Run evaluations
        # Skip step 0 evaluation if baseline was run (start_batch == 0 means baseline ran)
        if cfg.eval_every > 0 and i_batch % cfg.eval_every == 0 and not (i_batch == 0 and start_batch == 0):
            if len(evaluators) == 0:
                warning_text1 = "⚠ WARNING: No evaluators configured. Skipping evaluation. "
                logger.warning(f"║ {warning_text1}" + " " * (78 - len(warning_text1)) + "║")
                warning_text2 = "  To enable evaluations, configure eval_tasks in dataset builder. "
                logger.warning(f"║ {warning_text2}" + " " * (78 - len(warning_text2)) + "║")
            else:
                eval_text = f"Running evaluations (every {cfg.eval_every} steps)..."
                logger.info(f"║ {eval_text}" + " " * (78 - len(eval_text)) + "║")
                eval_start = time.time()
                with timed("run_evals", metrics):
                    eval_metrics = await run_evaluations_parallel(
                        evaluators, sampling_client, cfg, i_batch, model_path=model_path
                    )
                    metrics.update(eval_metrics)
                eval_time = time.time() - eval_start
                logger.info(f"║ ✓ Evaluations completed in {eval_time:.2f}s" + " " * (78 - len(f"✓ Evaluations completed in {eval_time:.2f}s")) + "║")
                for key, value in eval_metrics.items():
                    logger.info(f"║   {key}: {value}" + " " * (78 - len(f"  {key}: {value}")) + "║")
                
                # Save evaluation results to a separate JSON file for easy reference
                eval_results_file = Path(cfg.log_path) / f"eval_results_batch_{i_batch:06d}.json"
                eval_results = {
                    "batch": i_batch,
                    "evaluation_time_seconds": eval_time,
                    "timestamp": datetime.now().isoformat(),
                    "metrics": eval_metrics,
                }
                with open(eval_results_file, "w") as f:
                    json.dump(eval_results, f, indent=2)
                logger.info(f"║ Evaluation results saved to {eval_results_file.name}" + " " * (78 - len(f"Evaluation results saved to {eval_results_file.name}")) + "║")
                
                # Log evaluation metrics with comparison to previous step
                # Only show comparison if we have previous metrics
                ml_logger.log_metrics(eval_metrics, step=i_batch, previous_metrics=previous_eval_metrics if previous_eval_metrics else None)
                
                # Update previous_eval_metrics for next comparison
                previous_eval_metrics = eval_metrics.copy()

        # Get batch and sample trajectories
        batch_text = f"Getting batch {i_batch} from dataset..."
        logger.info(f"║ {batch_text}" + " " * (78 - len(batch_text)) + "║")
        batch_get_start = time.time()
        env_group_builders_P = dataset.get_batch(i_batch)
        batch_get_time = time.time() - batch_get_start
        logger.info(f"║ ✓ Retrieved {len(env_group_builders_P)} environment group builder(s) in {batch_get_time:.3f}s" + " " * (78 - len(f"✓ Retrieved {len(env_group_builders_P)} environment group builder(s) in {batch_get_time:.3f}s")) + "║")

        # Initialize logtree trace for this iteration if logging is enabled
        logger.info(f"║ Starting rollouts for {len(env_group_builders_P)} environment group(s)..." + " " * (78 - len(f"Starting rollouts for {len(env_group_builders_P)} environment group(s)...")) + "║")
        rollout_start = time.time()
        
        # Set rollout context if available (for CUA and other custom rollouts that support it)
        # This allows custom rollout functions to access current step/batch information
        try:
            from tinker_cookbook.recipes.cua_rl.core.rollout import set_rollout_context
            set_rollout_context(step=i_batch, batch=i_batch)
        except (ImportError, AttributeError):
            # set_rollout_context not available (not using CUA rollout), skip silently
            pass
        
        with _get_logtree_scope(
            log_path=cfg.log_path,
            num_groups_to_log=cfg.num_groups_to_log,
            f_name=f"train_iteration_{i_batch:06d}",
            scope_name=f"RL Iteration {i_batch}",
        ):
            # Note: do_remove_constant_reward_groups=False here because we remove
            # constant reward groups after all rollouts are collected (below)
            trajectory_groups_P = await asyncio.gather(
                *[
                    asyncio.create_task(
                        do_group_rollout_and_filter_constant_reward(
                            sampling_client,
                            builder,
                            max_tokens=cfg.max_tokens,
                            temperature=cfg.temperature,
                            do_remove_constant_reward_groups=False,
                            enable_logging=i < cfg.num_groups_to_log,
                            model_path=model_path,
                            group=i,  # Pass group index
                        ),
                        name=f"sample_task_{i}",
                    )
                    for i, builder in enumerate(env_group_builders_P)
                ],
            )
        rollout_time = time.time() - rollout_start
        logger.info(f"║ ✓ Rollouts completed in {rollout_time:.2f}s" + " " * (78 - len(f"✓ Rollouts completed in {rollout_time:.2f}s")) + "║")
        logger.info(f"║   Collected {len(trajectory_groups_P)} trajectory group(s)" + " " * (78 - len(f"  Collected {len(trajectory_groups_P)} trajectory group(s)")) + "║")

        if cfg.remove_constant_reward_groups:
            filter_start = time.time()
            filter_text = "Filtering constant reward groups..."
            logger.info(f"║ {filter_text}" + " " * (78 - len(filter_text)) + "║")
            trajectory_groups_P = remove_constant_reward_groups(trajectory_groups_P)
            filter_time = time.time() - filter_start
            logger.info(f"║ ✓ Filtering completed in {filter_time:.3f}s, {len(trajectory_groups_P)} groups remaining" + " " * (78 - len(f"✓ Filtering completed in {filter_time:.3f}s, {len(trajectory_groups_P)} groups remaining")) + "║")

        # Train step
        train_step_text = "Starting training step..."
        logger.info(f"║ {train_step_text}" + " " * (78 - len(train_step_text)) + "║")
        train_start = time.time()
        sampling_client, model_path, train_step_metrics = await do_train_step_and_get_sampling_client(
            cfg,
            i_batch,
            training_client,
            service_client,
            tokenizer,
            env_group_builders_P,
            trajectory_groups_P,
        )
        train_time = time.time() - train_start
        logger.info(f"║ ✓ Training step completed in {train_time:.2f}s" + " " * (78 - len(f"✓ Training step completed in {train_time:.2f}s")) + "║")
        logger.info(f"║   Updated model path: {model_path}" + " " * (78 - len(f"  Updated model path: {model_path}")) + "║")
        
        # Log key training metrics
        for key, value in train_step_metrics.items():
            if key.startswith("trajectory/") or key.startswith("train/"):
                logger.info(f"║   {key}: {value:.4f}" + " " * (78 - len(f"  {key}: {value:.4f}")) + "║")

        # Log metrics
        metrics.update(train_step_metrics)
        metrics["time/total"] = time.time() - t_start
        batch_total_time = time.time() - batch_start_time
        logger.info(f"║ Total batch time: {batch_total_time:.2f}s" + " " * (78 - len(f"Total batch time: {batch_total_time:.2f}s")) + "║")
        logger.info("╚" + "=" * 78 + "╝")
        
        # Log all metrics (without comparison for non-eval metrics)
        ml_logger.log_metrics(metrics, step=i_batch)


@scope
async def main(
    cfg: Config,
):
    """Main training loop for MDP RL."""
    # Force flush at the very beginning
    import sys
    sys.stdout.flush()
    sys.stderr.flush()
    
    logger.info("")
    logger.info("=" * 80)
    logger.info("TRAINING INITIALIZATION")
    logger.info("=" * 80)
    sys.stdout.flush()
    sys.stderr.flush()
    logger.info(f"Model: {cfg.model_name}")
    logger.info(f"LoRA rank: {cfg.lora_rank}")
    logger.info(f"Learning rate: {cfg.learning_rate}")
    logger.info(f"Max tokens: {cfg.max_tokens}")
    logger.info(f"Temperature: {cfg.temperature}")
    logger.info(f"Log path: {cfg.log_path}")
    
    ml_logger = ml_log.setup_logging(
        log_dir=cfg.log_path,
        wandb_project=cfg.wandb_project,
        config=cfg,
        wandb_name=cfg.wandb_name,
    )
    if cfg.enable_trace:
        # Get and rename the current (main) task
        current_task = asyncio.current_task()
        if current_task is not None:
            current_task.set_name("main")
        trace_events_path = os.path.join(cfg.log_path, "trace_events.jsonl")
        logger.info(f"Tracing is enabled. Trace events will be saved to {trace_events_path}")
        logger.info(
            f"Run `python tinker_cookbook/utils/trace.py {trace_events_path} trace.json` and visualize in chrome://tracing or https://ui.perfetto.dev/"
        )
        trace_init(output_file=trace_events_path)

    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("pylatexenc").setLevel(logging.WARNING)

    logger.info(f"[Init] Checking for existing checkpoints...")
    resume_info = checkpoint_utils.get_last_checkpoint(cfg.log_path)
    if resume_info:
        start_batch = resume_info["batch"]
        logger.info(f"[Init] ✓ Found checkpoint, resuming from batch {start_batch}")
    else:
        start_batch = 0
        logger.info(f"[Init] ✓ No checkpoint found, starting from batch 0")

    logger.info(f"[Init] Creating Tinker service client...")
    service_start = time.time()
    service_client = tinker.ServiceClient(base_url=cfg.base_url)
    service_time = time.time() - service_start
    logger.info(f"[Init] ✓ Service client created in {service_time:.3f}s")
    
    logger.info(f"[Init] Creating training client...")
    train_client_start = time.time()
    if resume_info:
        # Resuming interrupted training - load optimizer state for proper continuation
        training_client = (
            await service_client.create_training_client_from_state_with_optimizer_async(
                resume_info["state_path"]
            )
        )
        logger.info(f"[Init] ✓ Resumed training from {resume_info['state_path']}")
    elif cfg.load_checkpoint_path:
        # Starting fresh from a checkpoint - load weights only (fresh optimizer)
        training_client = await service_client.create_training_client_from_state_async(
            cfg.load_checkpoint_path
        )
        logger.info(f"[Init] ✓ Loaded weights from {cfg.load_checkpoint_path}")
    else:
        training_client = await service_client.create_lora_training_client_async(
            cfg.model_name, rank=cfg.lora_rank
        )
        logger.info(f"[Init] ✓ Created new LoRA training client (rank={cfg.lora_rank})")
    train_client_time = time.time() - train_client_start
    logger.info(f"[Init] Training client setup completed in {train_client_time:.3f}s")

    # Get tokenizer from training client
    logger.info(f"[Init] Getting tokenizer...")
    tokenizer = training_client.get_tokenizer()
    logger.info(f"[Init] ✓ Tokenizer ready")

    # Create dataset from thunk
    logger.info(f"[Init] Building dataset...")
    dataset_start = time.time()
    dataset, maybe_test_dataset = await cfg.dataset_builder()
    dataset_time = time.time() - dataset_start
    logger.info(f"[Init] ✓ Dataset built in {dataset_time:.3f}s")
    
    logger.info(f"[Init] Setting up evaluators...")
    evaluators = [evaluator() for evaluator in cfg.evaluator_builders]
    if maybe_test_dataset is not None:
        evaluators.append(RLTestSetEvaluator(maybe_test_dataset, max_tokens=cfg.max_tokens))
        logger.info(f"[Init] ✓ Added test set evaluator")
    logger.info(f"[Init] ✓ Total evaluators: {len(evaluators)}")
    
    # Warn if eval_every is set but no evaluators are configured
    if cfg.eval_every > 0 and len(evaluators) == 0:
        logger.warning(
            f"[Init] ⚠ WARNING: eval_every={cfg.eval_every} is set, but no evaluators are configured. "
            "Evaluations will be skipped. To enable evaluations, configure eval_tasks in your dataset builder "
            "or add evaluators via evaluator_builders."
        )

    num_batches = len(dataset)
    logger.info(f"[Init] Will train on {num_batches} batches")
    logger.info("=" * 80)

    # Run baseline evaluation before training starts (if starting from batch 0 and evaluators are configured)
    if start_batch == 0 and len(evaluators) > 0 and not cfg.skip_baseline:
        logger.info("")
        logger.info("=" * 80)
        logger.info("BASELINE EVALUATION (Before Training)")
        logger.info("=" * 80)
        logger.info("Running baseline evaluation to establish initial model performance...")
        
        # Get sampling client for baseline evaluation
        baseline_sampling_client, baseline_model_path, _ = await save_checkpoint_and_get_sampling_client(
            training_client, start_batch, cfg.log_path, cfg.save_every, start_batch
        )
        logger.info(f"[Baseline] Using model: {baseline_model_path}")
        
        # Set model_path for evaluation rollouts (for CUA custom rollout function)
        # Try to import set_eval_model_path from cua_rl.train if available
        try:
            from tinker_cookbook.recipes.cua_rl.train import set_eval_model_path
            set_eval_model_path(baseline_model_path)
        except ImportError:
            # Not using CUA RL, skip
            pass
        
        # Run baseline evaluation
        baseline_start = time.time()
        baseline_metrics = await run_baseline_evaluations_parallel(
            evaluators, baseline_sampling_client, cfg, model_path=baseline_model_path
        )
        
        # Clear model_path after evaluation
        try:
            from tinker_cookbook.recipes.cua_rl.train import set_eval_model_path
            set_eval_model_path(None)
        except ImportError:
            pass
        baseline_time = time.time() - baseline_start
        
        # Log baseline metrics
        logger.info(f"[Baseline] ✓ Baseline evaluation completed in {baseline_time:.2f}s")
        for key, value in baseline_metrics.items():
            logger.info(f"[Baseline] {key}: {value}")
        
        # Log to ML logger with step -1 to indicate baseline
        baseline_metrics_with_prefix = {f"baseline/{k}": v for k, v in baseline_metrics.items()}
        ml_logger.log_metrics(baseline_metrics_with_prefix, step=-1)
        
        # Save baseline evaluation results to a separate JSON file for easy reference
        baseline_results_file = Path(cfg.log_path) / "baseline_eval_results.json"
        baseline_results = {
            "step": -1,
            "model_path": baseline_model_path,
            "evaluation_time_seconds": baseline_time,
            "timestamp": datetime.now().isoformat(),
            "metrics": baseline_metrics,
        }
        with open(baseline_results_file, "w") as f:
            json.dump(baseline_results, f, indent=2)
        logger.info(f"[Baseline] Evaluation results saved to {baseline_results_file}")
        
        logger.info("=" * 80)
        logger.info("")

    # If baseline_only flag is set, exit after baseline evaluation
    if cfg.baseline_only:
        logger.info("")
        logger.info("=" * 80)
        logger.info("BASELINE ONLY MODE - Exiting after baseline evaluation")
        logger.info("=" * 80)
        logger.info("")
        return

    # Training loop
    if cfg.async_config is not None:
        training_func = do_async_training
    elif cfg.stream_minibatch_config is not None:
        training_func = do_sync_training_with_stream_minibatch
    else:
        training_func = do_sync_training
    await training_func(
        start_batch=start_batch,
        end_batch=num_batches,
        num_batches=num_batches,
        cfg=cfg,
        training_client=training_client,
        service_client=service_client,
        evaluators=evaluators,
        dataset=dataset,
        ml_logger=ml_logger,
        tokenizer=tokenizer,
    )

    # Save final checkpoint
    logger.info("")
    logger.info("=" * 80)
    logger.info("TRAINING COMPLETION")
    logger.info("=" * 80)
    if start_batch < num_batches:
        logger.info(f"[Final] Saving final checkpoint...")
        final_start = time.time()
        _ = await checkpoint_utils.save_checkpoint_async(
            training_client=training_client,
            name="final",
            log_path=cfg.log_path,
            kind="both",
            loop_state={"batch": num_batches},
        )
        final_time = time.time() - final_start
        logger.info(f"[Final] ✓ Final checkpoint saved in {final_time:.3f}s")
    else:
        logger.info("[Final] Training was already complete; nothing to do")

    # Cleanup
    logger.info(f"[Final] Cleaning up...")
    ml_logger.close()
    logger.info("=" * 80)
    logger.info("Training completed successfully")
    logger.info("=" * 80)
